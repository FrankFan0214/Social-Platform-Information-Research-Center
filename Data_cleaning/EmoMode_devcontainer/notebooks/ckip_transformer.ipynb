{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading config.json: 100%|██████████| 804/804 [00:00<00:00, 1.54MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 388M/388M [00:10<00:00, 38.5MB/s] \n",
      "Downloading tokenizer_config.json: 100%|██████████| 301/301 [00:00<00:00, 370kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 107k/107k [00:00<00:00, 7.57MB/s]\n",
      "Downloading special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 663kB/s]\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 9341.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['台灣', '位於', '東亞', '，', '是', '一', '個', '科技', '創新', '的', '重要', '基地', '。']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ckip_transformers.nlp import CkipWordSegmenter\n",
    "\n",
    "# 初始化斷詞器\n",
    "ws_driver = CkipWordSegmenter(model=\"bert-base\")\n",
    "\n",
    "# 測試文本\n",
    "text = [\"台灣位於東亞，是一個科技創新的重要基地。\"]\n",
    "ws_results = ws_driver(text)\n",
    "\n",
    "# 輸出結果\n",
    "for sentence in ws_results:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load models\n",
    "We provide several pretrained models for the NLP tools.\n",
    "我們提供了一些適用於自然語言工具的預訓練的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading config.json: 100%|██████████| 2.79k/2.79k [00:00<00:00, 12.1MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 388M/388M [00:11<00:00, 36.5MB/s] \n",
      "Downloading tokenizer_config.json: 100%|██████████| 301/301 [00:00<00:00, 619kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 107k/107k [00:00<00:00, 625kB/s] \n",
      "Downloading special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 689kB/s]\n",
      "Downloading config.json: 100%|██████████| 3.62k/3.62k [00:00<00:00, 5.50MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 388M/388M [00:10<00:00, 40.1MB/s] \n",
      "Downloading tokenizer_config.json: 100%|██████████| 301/301 [00:00<00:00, 491kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 107k/107k [00:00<00:00, 321kB/s] \n",
      "Downloading special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 312kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize drivers\n",
    "ws_driver  = CkipWordSegmenter(model=\"bert-base\")\n",
    "pos_driver = CkipPosTagger(model=\"bert-base\")\n",
    "ner_driver = CkipNerChunker(model=\"bert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以運用我們的工具於自己訓練的模型上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize drivers with custom checkpoints\n",
    "ws_driver  = CkipWordSegmenter(model_name=\"path_to_your_model\")\n",
    "pos_driver = CkipPosTagger(model_name=\"path_to_your_model\")\n",
    "ner_driver = CkipNerChunker(model_name=\"path_to_your_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可於宣告斷詞等工具時指定 device 以使用 GPU，設為 -1 （預設值）代表不使用 GPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CPU\n",
    "ws_driver = CkipWordSegmenter(device=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run pipeline\n",
    "斷詞與實體辨識的輸入必須是 list of sentences。\n",
    "詞性標記的輸入必須是 list of list of words。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 3/3 [00:00<00:00, 2360.77it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "Tokenization: 100%|██████████| 3/3 [00:00<00:00, 4340.43it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "Tokenization: 100%|██████████| 3/3 [00:00<00:00, 4208.33it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# Input text\n",
    "text = [\n",
    "   \"傅達仁今將執行安樂死，卻突然爆出自己20年前遭緯來體育台封殺，他不懂自己哪裡得罪到電視台。\",\n",
    "   \"美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會，預料她將會很順利通過參議院支持，成為該國有史以來第一位的華裔女性內閣成員。\",\n",
    "   \"空白 也是可以的～\",\n",
    "]\n",
    "\n",
    "# Run pipeline\n",
    "ws  = ws_driver(text)\n",
    "pos = pos_driver(ws)\n",
    "ner = ner_driver(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "詞性標記工具會自動用 '，,。：:；;！!？?' 等字元在執行模型前切割句子（輸出的句子會自動接回）。可設定 delim_set 參數使用別的字元做切割。\n",
    "另外可指定 use_delim=False 已停用此功能，或於斷詞、實體辨識時指定 use_delim=True 已啟用此功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 3/3 [00:00<00:00, 3199.32it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "Tokenization: 100%|██████████| 3/3 [00:00<00:00, 12384.76it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n",
      "Tokenization: 100%|██████████| 3/3 [00:00<00:00, 19784.45it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Enable sentence segmentation\n",
    "ws  = ws_driver(text, use_delim=True)\n",
    "ner = ner_driver(text, use_delim=True)\n",
    "\n",
    "# Disable sentence segmentation\n",
    "pos = pos_driver(ws, use_delim=False)\n",
    "\n",
    "# Use new line characters and tabs for sentence segmentation\n",
    "# pos = pos_driver(ws, delim_set='\\n\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您亦可設置 batch_size 與 max_length 以更完美的利用您的機器資源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the batch size and maximum sentence length\n",
    "ws = ws_driver(text, batch_size=256, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "傅達仁今將執行安樂死，卻突然爆出自己20年前遭緯來體育台封殺，他不懂自己哪裡得罪到電視台。\n",
      "傅達仁(Nb)　今(Nd)　將(D)　執行(VC)　安樂死(Na)　，(COMMACATEGORY)　卻(D)　突然(D)　爆出(VJ)　自己(Nh)　20(Neu)　年(Nd)　前(Ng)　遭(P)　緯來(Nb)　體育台(Na)　封殺(VC)　，(COMMACATEGORY)　他(Nh)　不(D)　懂(VK)　自己(Nh)　哪裡(Ncd)　得罪到(VC)　電視台(Nc)　。(PERIODCATEGORY)\n",
      "NerToken(word='傅達仁', ner='PERSON', idx=(0, 3))\n",
      "NerToken(word='20年', ner='DATE', idx=(18, 21))\n",
      "NerToken(word='緯來體育台', ner='ORG', idx=(23, 28))\n",
      "\n",
      "美國參議院針對今天總統布什所提名的勞工部長趙小蘭展開認可聽證會，預料她將會很順利通過參議院支持，成為該國有史以來第一位的華裔女性內閣成員。\n",
      "美國(Nc)　參議院(Nc)　針對(P)　今天(Nd)　總統(Na)　布什(Nb)　所(D)　提名(VC)　的(DE)　勞工部長(Na)　趙小蘭(Nb)　展開(VC)　認可(VC)　聽證會(Na)　，(COMMACATEGORY)　預料(VE)　她(Nh)　將(D)　會(D)　很(Dfa)　順利(VH)　通過(VC)　參議院(Nc)　支持(VC)　，(COMMACATEGORY)　成為(VG)　該(Nes)　國(Nc)　有史以來(D)　第一(Neu)　位(Nf)　的(DE)　華裔(Na)　女性(Na)　內閣(Na)　成員(Na)　。(PERIODCATEGORY)\n",
      "NerToken(word='美國參議院', ner='ORG', idx=(0, 5))\n",
      "NerToken(word='今天', ner='LOC', idx=(7, 9))\n",
      "NerToken(word='布什', ner='PERSON', idx=(11, 13))\n",
      "NerToken(word='勞工部長', ner='ORG', idx=(17, 21))\n",
      "NerToken(word='趙小蘭', ner='PERSON', idx=(21, 24))\n",
      "NerToken(word='認可聽證會', ner='EVENT', idx=(26, 31))\n",
      "NerToken(word='參議院', ner='ORG', idx=(42, 45))\n",
      "NerToken(word='第一', ner='ORDINAL', idx=(56, 58))\n",
      "NerToken(word='華裔', ner='NORP', idx=(60, 62))\n",
      "\n",
      "空白 也是可以的～\n",
      "空白(VH)　 (WHITESPACE)　也(D)　是(SHI)　可以(VH)　的(T)　～(FW)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pack word segmentation and part-of-speech results\n",
    "def pack_ws_pos_sentece(sentence_ws, sentence_pos):\n",
    "   assert len(sentence_ws) == len(sentence_pos)\n",
    "   res = []\n",
    "   for word_ws, word_pos in zip(sentence_ws, sentence_pos):\n",
    "      res.append(f\"{word_ws}({word_pos})\")\n",
    "   return \"\\u3000\".join(res)\n",
    "\n",
    "# Show results\n",
    "for sentence, sentence_ws, sentence_pos, sentence_ner in zip(text, ws, pos, ner):\n",
    "   print(sentence)\n",
    "   print(pack_ws_pos_sentece(sentence_ws, sentence_pos))\n",
    "   for entity in sentence_ner:\n",
    "      print(entity)\n",
    "   print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
